<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning Project Report</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        pre {
            background-color: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        .agent-section {
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .code-block {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            margin: 10px 0;
        }
        .highlight {
            background-color: #fff3cd;
            padding: 2px 5px;
            border-radius: 3px;
        }
    </style>
</head>
<body>
    <h1>Reinforcement Learning Assignment Report</h1>

    <section id="project_structure">
        <h2>1. Project Structure and Components</h2>
        <div class="code-block">
            <pre>
Project Organization:
/src
    /agents/         - Agent implementations (DQN, A2C)
    /constants/      - Game and training parameters
    /utils/          - Support utilities (Resource Manager, Vehicle Pool)
/assets
    /images/         - Game sprites (agent.png, vehicle.png)
    /sounds/         - Game audio (crash.ogg, win.ogg)
/models             - Trained model checkpoints
Main Files:
- main.py           - Single agent environment
- main_multi.py     - Multi-agent environment
- train.py          - Training orchestration
- train_a2c.py      - A2C specific training
- train_dqn.py      - DQN specific training
            </pre>
        </div>

        <h3>1.1 Environment Implementation</h3>
        <p>The project implements a vehicle navigation environment with the following key features:</p>
        <ul>
            <li><strong>Resource Management:</strong>
                <ul>
                    <li>Dynamic resource loading system for images and sounds</li>
                    <li>Efficient vehicle pool implementation for object reuse</li>
                    <li>Memory optimization through object pooling</li>
                </ul>
            </li>
            <li><strong>Vehicle System:</strong>
                <ul>
                    <li>Dynamic vehicle spawning and management</li>
                    <li>Multiple lane support with different speeds</li>
                    <li>Collision detection and handling</li>
                </ul>
            </li>
        </ul>
    </section>

    <section id="implementation_details">
        <h2>2. Detailed Implementation Analysis</h2>

        <h3>2.1 Vehicle Pool System</h3>
        <div class="code-block">
            <pre>
class VehiclePool:
    def __init__(self, initial_size=50):
        self.available_vehicles = []
        self.active_vehicles = set()
        
    Key Features:
    - Object pooling for performance optimization
    - Dynamic vehicle management
    - Resource efficient implementation
            </pre>
        </div>

        <h3>2.2 Agent Architectures</h3>
        <div class="code-block">
            <pre>
DQN Agent Components:
- Feature Network: State processing
- Q-Network: Action value estimation
- Experience Replay: (state, action, reward, next_state, done)
- Double Q-learning implementation

A2C Agent Components:
- Actor Network: Policy distribution
- Critic Network: State value estimation
- Advantage calculation
- Policy optimization
            </pre>
        </div>

        <h3>2.3 Training Systems</h3>
        <ul>
            <li><strong>Multiple Training Approaches:</strong>
                <ul>
                    <li>Single-agent training (train.py)</li>
                    <li>Multi-agent training (train_multi.py)</li>
                    <li>Algorithm-specific training (train_a2c.py, train_dqn.py)</li>
                </ul>
            </li>
            <li><strong>Logging and Monitoring:</strong>
                <ul>
                    <li>Episode rewards tracking</li>
                    <li>Performance metrics logging</li>
                    <li>Model checkpointing</li>
                </ul>
            </li>
        </ul>
    </section>

    <section id="model_analysis">
        <h2>3. Model Analysis and Performance</h2>

        <h3>3.1 DQN Implementation Details</h3>
        <div class="code-block">
            <pre>
Network Architecture:
- Input: Game state features
- Hidden layers with ReLU activation
- Output: Q-values for each action

Training Parameters:
- Learning rate: 1e-3
- Batch size: 64
- Replay buffer: 10,000 experiences
- Target network update: Every 100 steps
            </pre>
        </div>

        <h3>3.2 A2C Implementation Details</h3>
        <div class="code-block">
            <pre>
Actor-Critic Architecture:
- Shared feature extraction
- Policy head (Actor)
- Value head (Critic)

Training Configurations:
- Multiple variants implemented:
  * MC (Monte Carlo)
  * N-step returns
  * TD(λ) learning
- Advantage estimation with GAE
            </pre>
        </div>
    </section>

    <section id="experimental_results">
        <h2>4. Experimental Results and Analysis</h2>

        <h3>4.1 Training Progress</h3>
        <ul>
            <li><strong>Model Checkpoints:</strong>
                <ul>
                    <li>Best performing models saved (a2c_agent_best.pth, dqn_agent_best.pth)</li>
                    <li>Multiple checkpoints for different training stages</li>
                    <li>Performance comparison across different versions</li>
                </ul>
            </li>
            <li><strong>Learning Metrics:</strong>
                <ul>
                    <li>Episode rewards</li>
                    <li>Training stability</li>
                    <li>Convergence analysis</li>
                </ul>
            </li>
        </ul>

        <h3>4.2 Multi-Agent Performance</h3>
        <p>The project includes multi-agent training capabilities with:</p>
        <ul>
            <li>Parallel agent training</li>
            <li>Shared experience learning</li>
            <li>Scalable architecture</li>
        </ul>
    </section>

    <section id="overview">
        <h2>1. Project Overview</h2>
        <p>This assignment implements and analyzes fundamental reinforcement learning algorithms in the context of game control. The project explores two major approaches in modern RL: value-based learning through DQN and policy-based learning through A2C, demonstrating practical applications of theoretical concepts covered in the course.</p>

        <h3>1.1 Learning Objectives</h3>
        <ul>
            <li>Understanding and implementing core RL concepts:
                <ul>
                    <li>Value function approximation</li>
                    <li>Policy gradients</li>
                    <li>Temporal difference learning</li>
                    <li>Function approximation using neural networks</li>
                </ul>
            </li>
            <li>Practical implementation skills:
                <ul>
                    <li>PyTorch framework utilization</li>
                    <li>Neural network architecture design</li>
                    <li>Hyperparameter tuning</li>
                </ul>
            </li>
        </ul>

        <h3>1.2 Theoretical Foundation</h3>
        <div class="code-block">
            <pre>
Key Concepts Applied:
1. Markov Decision Process (MDP)
   - States: Game environment states
   - Actions: Available control options
   - Rewards: Performance feedback
   - Transitions: Environment dynamics

2. Value Functions
   - Q(s,a): Action-value function
   - V(s): State-value function
   - Advantage function: A(s,a) = Q(s,a) - V(s)

3. Learning Algorithms
   - Temporal Difference (TD) Learning
   - Policy Gradients
   - Experience Replay
            </pre>
        </div>
    </section>

    <section id="mathematical_foundations">
        <h2>2. Mathematical Foundations</h2>
        
        <h3>2.1 DQN Mathematical Framework</h3>
        <div class="code-block">
            <pre>
Q-Learning Update:
Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
where:
- α: Learning rate (1e-3)
- γ: Discount factor (0.99)
- r: Immediate reward
- s': Next state
- a': Next action

Loss Function:
L = E[(r + γ max Q(s',a') - Q(s,a))²]
            </pre>
        </div>

        <h3>2.2 A2C Mathematical Framework</h3>
        <div class="code-block">
            <pre>
Policy Gradient:
∇J(θ) = E[∇log π(a|s;θ) A(s,a)]
where:
- π(a|s;θ): Policy function
- A(s,a): Advantage function
- θ: Policy parameters

Advantage Estimation:
A(s,a) = r + γV(s') - V(s)
            </pre>
        </div>
    </section>

    <section id="environment">
        <h2>2. Environment Details</h2>
        <h3>2.1 State Space Analysis</h3>
        <ul>
            <li><strong>Dimensionality:</strong> Multi-dimensional continuous state space</li>
            <li><strong>State Components:</strong>
                <ul>
                    <li>Position variables (x, y coordinates)</li>
                    <li>Velocity information</li>
                    <li>Game-specific state indicators</li>
                </ul>
            </li>
        </ul>

        <h3>2.2 Action Space</h3>
        <ul>
            <li><strong>Type:</strong> Discrete action space</li>
            <li><strong>Actions:</strong>
                <ul>
                    <li>Movement controls (up, down, left, right)</li>
                    <li>Game-specific actions</li>
                </ul>
            </li>
        </ul>

        <h3>2.3 Reward System</h3>
        <div class="code-block">
            <pre>
Reward Structure:
- Immediate rewards for successful actions
- Penalty system for incorrect moves
- Terminal rewards for episode completion
- Intermediate rewards for progress indicators
            </pre>
        </div>
    </section>

    <section id="agents">
        <h2>3. Agent Implementations</h2>

        <div class="agent-section">
            <h3>3.1 Linear DQN Agent (Deep Analysis)</h3>
            <h4>3.1.1 Network Architecture</h4>
            <div class="code-block">
                <pre>
class LinearQNetwork:
    Input Layer (state_dim) 
    → Linear Layer 
    → Output Layer (action_dim)

Hyperparameters:
- Learning Rate: 1e-3
- Batch Size: 64
- Buffer Size: 10,000
- Gamma: 0.99
- Epsilon: 1.0 → 0.05 (decay=300)
                </pre>
            </div>

            <h4>3.1.2 Learning Process</h4>
            <ul>
                <li><strong>Experience Replay:</strong>
                    <ul>
                        <li>Stores (state, action, reward, next_state, done) tuples</li>
                        <li>Random sampling for decorrelated updates</li>
                        <li>Circular buffer implementation for memory efficiency</li>
                    </ul>
                </li>
                <li><strong>Double DQN Implementation:</strong>
                    <ul>
                        <li>Separate target network for stable Q-value estimation</li>
                        <li>Periodic target network updates</li>
                        <li>Reduced overestimation bias</li>
                    </ul>
                </li>
            </ul>
        </div>

        <div class="agent-section">
            <h3>3.2 A2C Agent (Detailed Analysis)</h3>
            <h4>3.2.1 Actor Network</h4>
            <div class="code-block">
                <pre>
Actor Architecture:
Input (state_dim) 
→ Linear Layer + ReLU 
→ Linear Layer 
→ Softmax (action probabilities)

Policy Updates:
- Policy Gradient with advantage estimation
- Entropy regularization for exploration
                </pre>
            </div>

            <h4>3.2.2 Critic Network</h4>
            <div class="code-block">
                <pre>
Critic Architecture:
Input (state_dim) 
→ Linear Layer + ReLU 
→ Linear Layer 
→ Value Estimate

Value Function Learning:
- TD(λ) learning
- GAE (Generalized Advantage Estimation)
                </pre>
            </div>
        </div>
    </section>

    <section id="training">
        <h2>4. Training Methodology</h2>
        
        <h3>4.1 DQN Training Process</h3>
        <ul>
            <li><strong>Exploration Strategy:</strong>
                <ul>
                    <li>Epsilon-greedy with decay schedule</li>
                    <li>Initial exploration phase</li>
                    <li>Gradual transition to exploitation</li>
                </ul>
            </li>
            <li><strong>Update Process:</strong>
                <ul>
                    <li>Batch sampling from replay buffer</li>
                    <li>Q-value updates using TD learning</li>
                    <li>Double Q-learning for action selection</li>
                </ul>
            </li>
        </ul>

        <h3>4.2 A2C Training Process</h3>
        <ul>
            <li><strong>Trajectory Collection:</strong>
                <ul>
                    <li>On-policy experience collection</li>
                    <li>Multi-step returns calculation</li>
                    <li>Advantage estimation</li>
                </ul>
            </li>
            <li><strong>Network Updates:</strong>
                <ul>
                    <li>Actor updates using policy gradients</li>
                    <li>Critic updates using TD errors</li>
                    <li>Synchronized learning of both networks</li>
                </ul>
            </li>
        </ul>
    </section>

    <section id="performance">
        <h2>5. Performance Metrics</h2>
        
        <h3>5.1 Learning Efficiency</h3>
        <ul>
            <li><strong>DQN Metrics:</strong>
                <ul>
                    <li>Average reward per episode</li>
                    <li>Q-value convergence analysis</li>
                    <li>Exploration rate decay impact</li>
                </ul>
            </li>
            <li><strong>A2C Metrics:</strong>
                <ul>
                    <li>Policy entropy evolution</li>
                    <li>Value function accuracy</li>
                    <li>Advantage estimation quality</li>
                </ul>
            </li>
        </ul>

        <h3>5.2 Comparative Analysis</h3>
        <div class="code-block">
            <pre>
Performance Comparison:
DQN:
- Faster initial learning
- More stable performance
- Better sample efficiency

A2C:
- Better final performance
- More flexible policy learning
- Better handling of complex scenarios
            </pre>
        </div>
    </section>

    <section id="conclusion">
        <h2>5. Conclusion and Academic Insights</h2>
        <p>This assignment demonstrates practical implementation of theoretical RL concepts, highlighting:</p>
        <ul>
            <li>Trade-offs between value-based and policy-based methods</li>
            <li>Importance of proper hyperparameter tuning</li>
            <li>Practical considerations in RL implementation</li>
            <li>Real-world applications of theoretical concepts</li>
        </ul>
    </section>

    <section id="references">
        <h2>6. References</h2>
        <ul>
            <li>Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</li>
            <li>Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. Nature.</li>
            <li>Course materials and lectures</li>
        </ul>
    </section>
</body>
</html>